{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Fraud from Enron Email\n",
    "### Enron Submission Free-Response Questions\n",
    "##### by Kai Sheng TEH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project is to use Machine Learning to identify *person of interest (POI)* in the Enron fraud case through the use of financial information and email text files available to the public.\n",
    "\n",
    "There are 146 entries and 20 features in the dataset. Of those features, 14 are financial features denoted in U.S. dollar while 5 are text features in terms of number of emails sent or received. \n",
    "\n",
    "There is also a feature named 'poi' which identify whether the person is idnetified as POI in the real world situation. 18 of them are POIs while 128 are non-POIs. It is hopeful that the financial and email information of poi individuals are different from the other value to serve as red flags.\n",
    "\n",
    "The features are being iterated to find out how many 'NaN' are there.\n",
    "\n",
    "| Features                  | NaN Count |\n",
    "| ------------------------- |:--------- |\n",
    "| loan_advances             | 142       |\n",
    "| director_fees             | 129       |\n",
    "| restricted_stock_deferred | 128       |\n",
    "| deferral_payments         | 107       |\n",
    "| deferred_income           | 97        |\n",
    "| long_term_incentive       | 80        |\n",
    "| bonus                     | 64        |\n",
    "| shared_receipt_with_poi   | 60        |\n",
    "| from_messages             | 60        |\n",
    "| from_poi_to_this_person   | 60        |\n",
    "| from_this_person_to_poi   | 60        |\n",
    "| to_messages               | 60        |\n",
    "| other                     | 53        |\n",
    "| salary                    | 51        |\n",
    "| expenses                  | 51        |\n",
    "| exercised_stock_options   | 44        |\n",
    "| restricted_stock          | 36        |\n",
    "| total_payments            | 21        |\n",
    "| total_stock_value         | 20        |\n",
    "| poi                       | 0         |\n",
    "\n",
    "Through exploratory data analyiss, an outlier was determined when plotting a scatterplot of bonus vs salary. Going through the pdf file, the extremely high salary and bonus is under the 'TOTAL' entry. It was removed together with 'The Travel Agenct in the Park' using the *data_dict.pop* function.  As such, only 144 entries are left in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 new features were created. They are:\n",
    "1. bonus-salary ratio (bonus / salary)\n",
    "2. from_this_person_to_poi percentage (from_this_person_to_poi / from_messages)\n",
    "3. from_poi_to_this_person percentage (from_poi_to_this_person / to_messages)\n",
    "\n",
    "If there person is a POI, the bonus to salary ratio might be very high. Similarly, POIs might also interact through emails more frequently among themselves.\n",
    "\n",
    "Scaling was done using MinMaxScaler to ensure the impact of each variable is not dominated by any single variables with high values such as bonus or salary.\n",
    "\n",
    "Select K Best and Decision Tree methods were used to determine the feature scores as below:\n",
    "\n",
    "| SelectKBest                   | Score         | Decision Tree                 | Score         |\n",
    "|:----------------------------- | :------------ |:----------------------------- | :------------ |\n",
    "| exercised_stock_options\t    | 24.81507973   | exercised_stock_options\t    | 0.216485631   |\n",
    "| total_stock_value\t            | 24.18289868\t| expenses\t                    | 0.180751044   |\n",
    "| bonus\t                        | 20.79225205\t| shared_receipt_with_poi\t    | 0.178337845   |\n",
    "| salary\t                    | 18.28968404\t| from_this_person_to_poi_ratio | 0.135602729   |\n",
    "| from_this_person_to_poi_ratio | 16.40971255\t| total_payments\t            | 0.115005291   |\n",
    "| deferred_income\t            |11.45847658\t| total_stock_value\t            | 0.055633364   |\n",
    "| long_term_incentive\t        |9.922186013\t| from_this_person_to_poi\t    | 0.047666667   |\n",
    "| restricted_stock\t            |9.212810622\t| long_term_incentive\t        | 0.04237037    |\n",
    "| total_payments\t            |8.77277773\t\t| restricted_stock\t            | 0.028147059   |\n",
    "| shared_receipt_with_poi\t    |8.589420732\t| salary\t                    | 0             |\n",
    "| loan_advances\t                |7.184055658\t| bonus\t                        | 0             |\n",
    "| expenses\t                    |6.094173311\t| deferral_payments\t            | 0             |\n",
    "| from_poi_to_this_person\t    |5.243449713\t| deferred_income\t            | 0             |\n",
    "| from_poi_to_this_person_ratio\t| 3.128091748\t| restricted_stock_deferred\t    | 0             |\n",
    "| from_this_person_to_poi\t    |2.382612108\t| loan_advances\t                | 0             |\n",
    "| director_fees\t                | 2.126327802\t| from_messages\t                | 0             |\n",
    "| to_messages\t                | 1.646341129\t| director_fees\t                | 0             |\n",
    "| deferral_payments\t            | 0.224611275\t| from_poi_to_this_person\t    | 0             |\n",
    "| from_messages\t                | 0.169700948\t| from_poi_to_this_person_ratio | 0             |\n",
    "| restricted_stock_deferred\t    | 0.065499653   | salary_bonus_ratio\t        | 0             |\n",
    "| salary_bonus_ratio\t        | 0.000368768\t| to_messages\t                | 0             |\n",
    "\n",
    "SelectKBest scoring is chosen over DecisionTree.\n",
    "The top 10 features with the highest scores are kept for use in POI identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms that I tried are Naive-Bayes Gaussian, AdaBoost, Decision Tree, K Nearest Neighbors and Random Forest classifiers.\n",
    "\n",
    "At this point of time, only Gaussian and Decision Tree algorithms fulfil the >0.3 requirement for bothprecision and recall. Thus, the othersalgorithms need tuning in order to achieve >0.3 requirements.\n",
    "\n",
    "The performances of each algorithm before tuning are as below:\n",
    "\n",
    "| Algorithm            | Accuracy | Precision | Recall   | F1 Score |\n",
    "| :------------------- | :------- | :-------- | :------- | :------- |\n",
    "| Naive-Bayes Gaussian | 0.83613  | 0.36639   | 0.31400  | 0.33818  |\n",
    "| Decision Tree        | 0.82207  | 0.32459   | 0.30950  | 0.31687  |\n",
    "| AdaBoost             | 0.83853  | 0.36405   | 0.28250  | 0.31813  |\n",
    "| K Neartest Neighbors | 0.87640  | 0.63878   | 0.16800  | 0.26603  |\n",
    "| Random Forest        | 0.86140  | 0.44537   | 0.16100  | 0.23650  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parameters of an algorithm means changing the input parameters and comparing the performance of each parameter combination in order to determine the optimal parameters to be used. Parameters of algorithms not optimized will result in sub-optimized performance or lower accuracy.\n",
    "\n",
    "GridSearchCV was used to tune and find the best combination of parameters to be used in each algorithm.\n",
    "\n",
    "Gaussian classifiers doesn't have any parameters to tune and is thus not applicable in this case.\n",
    "\n",
    "The performances after tuning are being compared below:\n",
    "\n",
    "| Algorithm            | Accuracy | Precision | Recall   | F1 Score | Notes       |\n",
    "| :------------------- | :------- | :-------- | :------- | :------- | :---------- |\n",
    "| Naive-Bayes Gaussian | 0.83613  | 0.36639   | 0.31400  | 0.33818  | Tuning N/A  |\n",
    "| Decision Tree        | 0.82533  | 0.33351   | 0.31050  | 0.32160  | -           |\n",
    "| AdaBoost             | 0.83820  | 0.36305   | 0.28300  | 0.31807  | -           |\n",
    "| K Neartest Neighbors | 0.87640  | 0.63878   | 0.16800  | 0.26603  | -           |\n",
    "| Random Forest        | 0.86087  | 0.44299   | 0.16900  | 0.24466  | -           |\n",
    "\n",
    "Gaussian has the best best performances despite not needing any tunings. It is chosen as the POI identifier algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is the process of training the algorithms and testing how well the model has been trained.\n",
    "\n",
    "Few of the classic mistake made in validation are overfitting and using same training and testing data. Overfitting occurs when the algorithm is being fit too closely to the training data. It performs well on training data but poorly on testing data. As such, there must be a balanced between fitting too closely or loosely to training data so that the algorithms is able to perform a reasonably well generalization when being tested on unseen data.\n",
    "\n",
    "At the same time, training and testing should not be done on the same data. Instead, 10%-30% of the data should be set aside for testing. This will prevent overfitting as well.\n",
    "\n",
    "For this projct, *StratifiedShuffleSplit* is being used for validation in *tester.py*. The data are split into 1000 different sets, which is also known as folds. This allows us to maximize the amount of data available on hand to be used for training and testing. The performance is then averaged out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 evaluation metrics being used etensively in this project are precision, recall and F1 score.\n",
    "The average precision, recall and F1 score for Gaussian algorithm are 0.36639, 0.31400 and 0.33818 respectively.\n",
    "\n",
    "The definition of each evaluation metrics are as below:\n",
    "1. Precision = true positive / false positive (high precision -> only guilty person indicted, no false alarm)\n",
    "2. Recall = true positive / false negative (high recall -> guilty person are all indicted, no guilty person escapes)\n",
    "3. F1 score - a weighted average of precision and recall by multiplying the product of recall and precision by 2 then dividing the sum of precision and recall\n",
    "\n",
    "In this case, we would want to strike for a higher precision so as to not accuse the wrong person. Machine Learning is only one of the investigation methods and police or prosecutors should not rely wholly on machine learning. Any guilty person not being picked up by th algorithms should still be subjected to thorough investigations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
